using crate alpha
backbone.cls_token torch.Size([1, 1, 768])
backbone.pos_embed torch.Size([1, 1025, 768])
backbone.patch_embed.proj.weight torch.Size([768, 3, 16, 16])
backbone.patch_embed.proj.bias torch.Size([768])
backbone.blocks.0.gamma_1 torch.Size([768])
backbone.blocks.0.gamma_2 torch.Size([768])
backbone.blocks.0.norm1.weight torch.Size([768])
backbone.blocks.0.norm1.bias torch.Size([768])
backbone.blocks.0.attn.relative_position_bias_table torch.Size([3972, 12])
backbone.blocks.0.attn.relative_position_index torch.Size([1025, 1025])
backbone.blocks.0.attn.qkv.weight torch.Size([768, 768])
backbone.blocks.0.attn.to_out.0.weight torch.Size([768, 768])
backbone.blocks.0.norm2.weight torch.Size([768])
backbone.blocks.0.norm2.bias torch.Size([768])
backbone.blocks.0.mlp.D torch.Size([768, 3072])
backbone.blocks.0.mlp.D1 torch.Size([768, 3072])
backbone.blocks.1.gamma_1 torch.Size([768])
backbone.blocks.1.gamma_2 torch.Size([768])
backbone.blocks.1.norm1.weight torch.Size([768])
backbone.blocks.1.norm1.bias torch.Size([768])
backbone.blocks.1.attn.relative_position_bias_table torch.Size([3972, 12])
backbone.blocks.1.attn.relative_position_index torch.Size([1025, 1025])
backbone.blocks.1.attn.qkv.weight torch.Size([768, 768])
backbone.blocks.1.attn.to_out.0.weight torch.Size([768, 768])
backbone.blocks.1.norm2.weight torch.Size([768])
backbone.blocks.1.norm2.bias torch.Size([768])
backbone.blocks.1.mlp.D torch.Size([768, 3072])
backbone.blocks.1.mlp.D1 torch.Size([768, 3072])
backbone.blocks.2.gamma_1 torch.Size([768])
backbone.blocks.2.gamma_2 torch.Size([768])
backbone.blocks.2.norm1.weight torch.Size([768])
backbone.blocks.2.norm1.bias torch.Size([768])
backbone.blocks.2.attn.relative_position_bias_table torch.Size([3972, 12])
backbone.blocks.2.attn.relative_position_index torch.Size([1025, 1025])
backbone.blocks.2.attn.qkv.weight torch.Size([768, 768])
backbone.blocks.2.attn.to_out.0.weight torch.Size([768, 768])
backbone.blocks.2.norm2.weight torch.Size([768])
backbone.blocks.2.norm2.bias torch.Size([768])
backbone.blocks.2.mlp.D torch.Size([768, 3072])
backbone.blocks.2.mlp.D1 torch.Size([768, 3072])
backbone.blocks.3.gamma_1 torch.Size([768])
backbone.blocks.3.gamma_2 torch.Size([768])
backbone.blocks.3.norm1.weight torch.Size([768])
backbone.blocks.3.norm1.bias torch.Size([768])
backbone.blocks.3.attn.relative_position_bias_table torch.Size([3972, 12])
backbone.blocks.3.attn.relative_position_index torch.Size([1025, 1025])
backbone.blocks.3.attn.qkv.weight torch.Size([768, 768])
backbone.blocks.3.attn.to_out.0.weight torch.Size([768, 768])
backbone.blocks.3.norm2.weight torch.Size([768])
backbone.blocks.3.norm2.bias torch.Size([768])
backbone.blocks.3.mlp.D torch.Size([768, 3072])
backbone.blocks.3.mlp.D1 torch.Size([768, 3072])
backbone.blocks.4.gamma_1 torch.Size([768])
backbone.blocks.4.gamma_2 torch.Size([768])
backbone.blocks.4.norm1.weight torch.Size([768])
backbone.blocks.4.norm1.bias torch.Size([768])
backbone.blocks.4.attn.relative_position_bias_table torch.Size([3972, 12])
backbone.blocks.4.attn.relative_position_index torch.Size([1025, 1025])
backbone.blocks.4.attn.qkv.weight torch.Size([768, 768])
backbone.blocks.4.attn.to_out.0.weight torch.Size([768, 768])
backbone.blocks.4.norm2.weight torch.Size([768])
backbone.blocks.4.norm2.bias torch.Size([768])
backbone.blocks.4.mlp.D torch.Size([768, 3072])
backbone.blocks.4.mlp.D1 torch.Size([768, 3072])
backbone.blocks.5.gamma_1 torch.Size([768])
backbone.blocks.5.gamma_2 torch.Size([768])
backbone.blocks.5.norm1.weight torch.Size([768])
backbone.blocks.5.norm1.bias torch.Size([768])
backbone.blocks.5.attn.relative_position_bias_table torch.Size([3972, 12])
backbone.blocks.5.attn.relative_position_index torch.Size([1025, 1025])
backbone.blocks.5.attn.qkv.weight torch.Size([768, 768])
backbone.blocks.5.attn.to_out.0.weight torch.Size([768, 768])
backbone.blocks.5.norm2.weight torch.Size([768])
backbone.blocks.5.norm2.bias torch.Size([768])
backbone.blocks.5.mlp.D torch.Size([768, 3072])
backbone.blocks.5.mlp.D1 torch.Size([768, 3072])
backbone.blocks.6.gamma_1 torch.Size([768])
backbone.blocks.6.gamma_2 torch.Size([768])
backbone.blocks.6.norm1.weight torch.Size([768])
backbone.blocks.6.norm1.bias torch.Size([768])
backbone.blocks.6.attn.relative_position_bias_table torch.Size([3972, 12])
backbone.blocks.6.attn.relative_position_index torch.Size([1025, 1025])
backbone.blocks.6.attn.qkv.weight torch.Size([768, 768])
backbone.blocks.6.attn.to_out.0.weight torch.Size([768, 768])
backbone.blocks.6.norm2.weight torch.Size([768])
backbone.blocks.6.norm2.bias torch.Size([768])
backbone.blocks.6.mlp.D torch.Size([768, 3072])
backbone.blocks.6.mlp.D1 torch.Size([768, 3072])
backbone.blocks.7.gamma_1 torch.Size([768])
backbone.blocks.7.gamma_2 torch.Size([768])
backbone.blocks.7.norm1.weight torch.Size([768])
backbone.blocks.7.norm1.bias torch.Size([768])
backbone.blocks.7.attn.relative_position_bias_table torch.Size([3972, 12])
backbone.blocks.7.attn.relative_position_index torch.Size([1025, 1025])
backbone.blocks.7.attn.qkv.weight torch.Size([768, 768])
backbone.blocks.7.attn.to_out.0.weight torch.Size([768, 768])
backbone.blocks.7.norm2.weight torch.Size([768])
backbone.blocks.7.norm2.bias torch.Size([768])
backbone.blocks.7.mlp.D torch.Size([768, 3072])
backbone.blocks.7.mlp.D1 torch.Size([768, 3072])
backbone.blocks.8.gamma_1 torch.Size([768])
backbone.blocks.8.gamma_2 torch.Size([768])
backbone.blocks.8.norm1.weight torch.Size([768])
backbone.blocks.8.norm1.bias torch.Size([768])
backbone.blocks.8.attn.relative_position_bias_table torch.Size([3972, 12])
backbone.blocks.8.attn.relative_position_index torch.Size([1025, 1025])
backbone.blocks.8.attn.qkv.weight torch.Size([768, 768])
backbone.blocks.8.attn.to_out.0.weight torch.Size([768, 768])
backbone.blocks.8.norm2.weight torch.Size([768])
backbone.blocks.8.norm2.bias torch.Size([768])
backbone.blocks.8.mlp.D torch.Size([768, 3072])
backbone.blocks.8.mlp.D1 torch.Size([768, 3072])
backbone.blocks.9.gamma_1 torch.Size([768])
backbone.blocks.9.gamma_2 torch.Size([768])
backbone.blocks.9.norm1.weight torch.Size([768])
backbone.blocks.9.norm1.bias torch.Size([768])
backbone.blocks.9.attn.relative_position_bias_table torch.Size([3972, 12])
backbone.blocks.9.attn.relative_position_index torch.Size([1025, 1025])
backbone.blocks.9.attn.qkv.weight torch.Size([768, 768])
backbone.blocks.9.attn.to_out.0.weight torch.Size([768, 768])
backbone.blocks.9.norm2.weight torch.Size([768])
backbone.blocks.9.norm2.bias torch.Size([768])
backbone.blocks.9.mlp.D torch.Size([768, 3072])
backbone.blocks.9.mlp.D1 torch.Size([768, 3072])
backbone.blocks.10.gamma_1 torch.Size([768])
backbone.blocks.10.gamma_2 torch.Size([768])
backbone.blocks.10.norm1.weight torch.Size([768])
backbone.blocks.10.norm1.bias torch.Size([768])
backbone.blocks.10.attn.relative_position_bias_table torch.Size([3972, 12])
backbone.blocks.10.attn.relative_position_index torch.Size([1025, 1025])
backbone.blocks.10.attn.qkv.weight torch.Size([768, 768])
backbone.blocks.10.attn.to_out.0.weight torch.Size([768, 768])
backbone.blocks.10.norm2.weight torch.Size([768])
backbone.blocks.10.norm2.bias torch.Size([768])
backbone.blocks.10.mlp.D torch.Size([768, 3072])
backbone.blocks.10.mlp.D1 torch.Size([768, 3072])
backbone.blocks.11.gamma_1 torch.Size([768])
backbone.blocks.11.gamma_2 torch.Size([768])
backbone.blocks.11.norm1.weight torch.Size([768])
backbone.blocks.11.norm1.bias torch.Size([768])
backbone.blocks.11.attn.relative_position_bias_table torch.Size([3972, 12])
backbone.blocks.11.attn.relative_position_index torch.Size([1025, 1025])
backbone.blocks.11.attn.qkv.weight torch.Size([768, 768])
backbone.blocks.11.attn.to_out.0.weight torch.Size([768, 768])
backbone.blocks.11.norm2.weight torch.Size([768])
backbone.blocks.11.norm2.bias torch.Size([768])
backbone.blocks.11.mlp.D torch.Size([768, 3072])
backbone.blocks.11.mlp.D1 torch.Size([768, 3072])
backbone.fpn1.0.weight torch.Size([768, 768, 2, 2])
backbone.fpn1.0.bias torch.Size([768])
backbone.fpn1.1.weight torch.Size([768])
backbone.fpn1.1.bias torch.Size([768])
backbone.fpn1.1.running_mean torch.Size([768])
backbone.fpn1.1.running_var torch.Size([768])
backbone.fpn1.1.num_batches_tracked torch.Size([])
backbone.fpn1.3.weight torch.Size([768, 768, 2, 2])
backbone.fpn1.3.bias torch.Size([768])
backbone.fpn2.0.weight torch.Size([768, 768, 2, 2])
backbone.fpn2.0.bias torch.Size([768])
decode_head.conv_seg.weight torch.Size([150, 768, 1, 1])
decode_head.conv_seg.bias torch.Size([150])
decode_head.psp_modules.0.1.conv.weight torch.Size([768, 768, 1, 1])
decode_head.psp_modules.0.1.bn.weight torch.Size([768])
decode_head.psp_modules.0.1.bn.bias torch.Size([768])
decode_head.psp_modules.0.1.bn.running_mean torch.Size([768])
decode_head.psp_modules.0.1.bn.running_var torch.Size([768])
decode_head.psp_modules.0.1.bn.num_batches_tracked torch.Size([])
decode_head.psp_modules.1.1.conv.weight torch.Size([768, 768, 1, 1])
decode_head.psp_modules.1.1.bn.weight torch.Size([768])
decode_head.psp_modules.1.1.bn.bias torch.Size([768])
decode_head.psp_modules.1.1.bn.running_mean torch.Size([768])
decode_head.psp_modules.1.1.bn.running_var torch.Size([768])
decode_head.psp_modules.1.1.bn.num_batches_tracked torch.Size([])
decode_head.psp_modules.2.1.conv.weight torch.Size([768, 768, 1, 1])
decode_head.psp_modules.2.1.bn.weight torch.Size([768])
decode_head.psp_modules.2.1.bn.bias torch.Size([768])
decode_head.psp_modules.2.1.bn.running_mean torch.Size([768])
decode_head.psp_modules.2.1.bn.running_var torch.Size([768])
decode_head.psp_modules.2.1.bn.num_batches_tracked torch.Size([])
decode_head.psp_modules.3.1.conv.weight torch.Size([768, 768, 1, 1])
decode_head.psp_modules.3.1.bn.weight torch.Size([768])
decode_head.psp_modules.3.1.bn.bias torch.Size([768])
decode_head.psp_modules.3.1.bn.running_mean torch.Size([768])
decode_head.psp_modules.3.1.bn.running_var torch.Size([768])
decode_head.psp_modules.3.1.bn.num_batches_tracked torch.Size([])
decode_head.bottleneck.conv.weight torch.Size([768, 3840, 3, 3])
decode_head.bottleneck.bn.weight torch.Size([768])
decode_head.bottleneck.bn.bias torch.Size([768])
decode_head.bottleneck.bn.running_mean torch.Size([768])
decode_head.bottleneck.bn.running_var torch.Size([768])
decode_head.bottleneck.bn.num_batches_tracked torch.Size([])
decode_head.lateral_convs.0.conv.weight torch.Size([768, 768, 1, 1])
decode_head.lateral_convs.0.bn.weight torch.Size([768])
decode_head.lateral_convs.0.bn.bias torch.Size([768])
decode_head.lateral_convs.0.bn.running_mean torch.Size([768])
decode_head.lateral_convs.0.bn.running_var torch.Size([768])
decode_head.lateral_convs.0.bn.num_batches_tracked torch.Size([])
decode_head.lateral_convs.1.conv.weight torch.Size([768, 768, 1, 1])
decode_head.lateral_convs.1.bn.weight torch.Size([768])
decode_head.lateral_convs.1.bn.bias torch.Size([768])
decode_head.lateral_convs.1.bn.running_mean torch.Size([768])
decode_head.lateral_convs.1.bn.running_var torch.Size([768])
decode_head.lateral_convs.1.bn.num_batches_tracked torch.Size([])
decode_head.lateral_convs.2.conv.weight torch.Size([768, 768, 1, 1])
decode_head.lateral_convs.2.bn.weight torch.Size([768])
decode_head.lateral_convs.2.bn.bias torch.Size([768])
decode_head.lateral_convs.2.bn.running_mean torch.Size([768])
decode_head.lateral_convs.2.bn.running_var torch.Size([768])
decode_head.lateral_convs.2.bn.num_batches_tracked torch.Size([])
decode_head.fpn_convs.0.conv.weight torch.Size([768, 768, 3, 3])
decode_head.fpn_convs.0.bn.weight torch.Size([768])
decode_head.fpn_convs.0.bn.bias torch.Size([768])
decode_head.fpn_convs.0.bn.running_mean torch.Size([768])
decode_head.fpn_convs.0.bn.running_var torch.Size([768])
decode_head.fpn_convs.0.bn.num_batches_tracked torch.Size([])
decode_head.fpn_convs.1.conv.weight torch.Size([768, 768, 3, 3])
decode_head.fpn_convs.1.bn.weight torch.Size([768])
decode_head.fpn_convs.1.bn.bias torch.Size([768])
decode_head.fpn_convs.1.bn.running_mean torch.Size([768])
decode_head.fpn_convs.1.bn.running_var torch.Size([768])
decode_head.fpn_convs.1.bn.num_batches_tracked torch.Size([])
decode_head.fpn_convs.2.conv.weight torch.Size([768, 768, 3, 3])
decode_head.fpn_convs.2.bn.weight torch.Size([768])
decode_head.fpn_convs.2.bn.bias torch.Size([768])
decode_head.fpn_convs.2.bn.running_mean torch.Size([768])
decode_head.fpn_convs.2.bn.running_var torch.Size([768])
decode_head.fpn_convs.2.bn.num_batches_tracked torch.Size([])
decode_head.fpn_bottleneck.conv.weight torch.Size([768, 3072, 3, 3])
decode_head.fpn_bottleneck.bn.weight torch.Size([768])
decode_head.fpn_bottleneck.bn.bias torch.Size([768])
decode_head.fpn_bottleneck.bn.running_mean torch.Size([768])
decode_head.fpn_bottleneck.bn.running_var torch.Size([768])
decode_head.fpn_bottleneck.bn.num_batches_tracked torch.Size([])
auxiliary_head.conv_seg.weight torch.Size([150, 256, 1, 1])
auxiliary_head.conv_seg.bias torch.Size([150])
auxiliary_head.convs.0.conv.weight torch.Size([256, 768, 3, 3])
auxiliary_head.convs.0.bn.weight torch.Size([256])
auxiliary_head.convs.0.bn.bias torch.Size([256])
auxiliary_head.convs.0.bn.running_mean torch.Size([256])
auxiliary_head.convs.0.bn.running_var torch.Size([256])
auxiliary_head.convs.0.bn.num_batches_tracked torch.Size([])



load checkpoint

odict_keys(['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.gamma_1', 'blocks.0.gamma_2', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.relative_position_bias_table', 'blocks.0.attn.relative_position_index', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.to_out.0.weight', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.D', 'blocks.0.mlp.D1', 'blocks.1.gamma_1', 'blocks.1.gamma_2', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.relative_position_bias_table', 'blocks.1.attn.relative_position_index', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.to_out.0.weight', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.D', 'blocks.1.mlp.D1', 'blocks.2.gamma_1', 'blocks.2.gamma_2', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.relative_position_bias_table', 'blocks.2.attn.relative_position_index', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.to_out.0.weight', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.D', 'blocks.2.mlp.D1', 'blocks.3.gamma_1', 'blocks.3.gamma_2', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.relative_position_bias_table', 'blocks.3.attn.relative_position_index', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.to_out.0.weight', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.D', 'blocks.3.mlp.D1', 'blocks.4.gamma_1', 'blocks.4.gamma_2', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.relative_position_bias_table', 'blocks.4.attn.relative_position_index', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.to_out.0.weight', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.D', 'blocks.4.mlp.D1', 'blocks.5.gamma_1', 'blocks.5.gamma_2', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.relative_position_bias_table', 'blocks.5.attn.relative_position_index', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.to_out.0.weight', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.D', 'blocks.5.mlp.D1', 'blocks.6.gamma_1', 'blocks.6.gamma_2', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.relative_position_bias_table', 'blocks.6.attn.relative_position_index', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.to_out.0.weight', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.D', 'blocks.6.mlp.D1', 'blocks.7.gamma_1', 'blocks.7.gamma_2', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.relative_position_bias_table', 'blocks.7.attn.relative_position_index', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.to_out.0.weight', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.D', 'blocks.7.mlp.D1', 'blocks.8.gamma_1', 'blocks.8.gamma_2', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.relative_position_bias_table', 'blocks.8.attn.relative_position_index', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.to_out.0.weight', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.D', 'blocks.8.mlp.D1', 'blocks.9.gamma_1', 'blocks.9.gamma_2', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.relative_position_bias_table', 'blocks.9.attn.relative_position_index', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.to_out.0.weight', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.D', 'blocks.9.mlp.D1', 'blocks.10.gamma_1', 'blocks.10.gamma_2', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.relative_position_bias_table', 'blocks.10.attn.relative_position_index', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.to_out.0.weight', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.D', 'blocks.10.mlp.D1', 'blocks.11.gamma_1', 'blocks.11.gamma_2', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.relative_position_bias_table', 'blocks.11.attn.relative_position_index', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.to_out.0.weight', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.D', 'blocks.11.mlp.D1', 'fpn1.0.weight', 'fpn1.0.bias', 'fpn1.1.weight', 'fpn1.1.bias', 'fpn1.1.running_mean', 'fpn1.1.running_var', 'fpn1.1.num_batches_tracked', 'fpn1.3.weight', 'fpn1.3.bias', 'fpn2.0.weight', 'fpn2.0.bias'])



MAE(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (attend): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (qkv): Linear(in_features=768, out_features=768, bias=False)
        (to_out): Sequential(
          (0): Linear(in_features=768, out_features=768, bias=False)
        )
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp()
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (attend): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (qkv): Linear(in_features=768, out_features=768, bias=False)
        (to_out): Sequential(
          (0): Linear(in_features=768, out_features=768, bias=False)
        )
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp()
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (attend): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (qkv): Linear(in_features=768, out_features=768, bias=False)
        (to_out): Sequential(
          (0): Linear(in_features=768, out_features=768, bias=False)
        )
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp()
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (attend): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (qkv): Linear(in_features=768, out_features=768, bias=False)
        (to_out): Sequential(
          (0): Linear(in_features=768, out_features=768, bias=False)
        )
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp()
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (attend): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (qkv): Linear(in_features=768, out_features=768, bias=False)
        (to_out): Sequential(
          (0): Linear(in_features=768, out_features=768, bias=False)
        )
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp()
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (attend): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (qkv): Linear(in_features=768, out_features=768, bias=False)
        (to_out): Sequential(
          (0): Linear(in_features=768, out_features=768, bias=False)
        )
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp()
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (attend): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (qkv): Linear(in_features=768, out_features=768, bias=False)
        (to_out): Sequential(
          (0): Linear(in_features=768, out_features=768, bias=False)
        )
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp()
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (attend): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (qkv): Linear(in_features=768, out_features=768, bias=False)
        (to_out): Sequential(
          (0): Linear(in_features=768, out_features=768, bias=False)
        )
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp()
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (attend): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (qkv): Linear(in_features=768, out_features=768, bias=False)
        (to_out): Sequential(
          (0): Linear(in_features=768, out_features=768, bias=False)
        )
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp()
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (attend): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (qkv): Linear(in_features=768, out_features=768, bias=False)
        (to_out): Sequential(
          (0): Linear(in_features=768, out_features=768, bias=False)
        )
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp()
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (attend): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (qkv): Linear(in_features=768, out_features=768, bias=False)
        (to_out): Sequential(
          (0): Linear(in_features=768, out_features=768, bias=False)
        )
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp()
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (attend): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (qkv): Linear(in_features=768, out_features=768, bias=False)
        (to_out): Sequential(
          (0): Linear(in_features=768, out_features=768, bias=False)
        )
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp()
    )
  )
  (fpn1): Sequential(
    (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))
    (1): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): GELU()
    (3): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))
  )
  (fpn2): Sequential(
    (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))
  )
  (fpn3): Identity()
  (fpn4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)


#vanilla crate B/32

cls_token torch.Size([1, 1, 768])
pos_embed torch.Size([1, 257, 768])
patch_embed.proj.weight torch.Size([768, 3, 32, 32])
patch_embed.proj.bias torch.Size([768])
blocks.0.gamma_1 torch.Size([768])
blocks.0.gamma_2 torch.Size([768])
blocks.0.norm1.weight torch.Size([768])
blocks.0.norm1.bias torch.Size([768])
blocks.0.attn.relative_position_bias_table torch.Size([964, 12])
blocks.0.attn.relative_position_index torch.Size([257, 257])
blocks.0.attn.qkv.weight torch.Size([768, 768])
blocks.0.attn.to_out.0.weight torch.Size([768, 768])
blocks.0.norm2.weight torch.Size([768])
blocks.0.norm2.bias torch.Size([768])
blocks.0.mlp.D torch.Size([768, 768])
blocks.1.gamma_1 torch.Size([768])
blocks.1.gamma_2 torch.Size([768])
blocks.1.norm1.weight torch.Size([768])
blocks.1.norm1.bias torch.Size([768])
blocks.1.attn.relative_position_bias_table torch.Size([964, 12])
blocks.1.attn.relative_position_index torch.Size([257, 257])
blocks.1.attn.qkv.weight torch.Size([768, 768])
blocks.1.attn.to_out.0.weight torch.Size([768, 768])
blocks.1.norm2.weight torch.Size([768])
blocks.1.norm2.bias torch.Size([768])
blocks.1.mlp.D torch.Size([768, 768])
blocks.2.gamma_1 torch.Size([768])
blocks.2.gamma_2 torch.Size([768])
blocks.2.norm1.weight torch.Size([768])
blocks.2.norm1.bias torch.Size([768])
blocks.2.attn.relative_position_bias_table torch.Size([964, 12])
blocks.2.attn.relative_position_index torch.Size([257, 257])
blocks.2.attn.qkv.weight torch.Size([768, 768])
blocks.2.attn.to_out.0.weight torch.Size([768, 768])
blocks.2.norm2.weight torch.Size([768])
blocks.2.norm2.bias torch.Size([768])
blocks.2.mlp.D torch.Size([768, 768])
blocks.3.gamma_1 torch.Size([768])
blocks.3.gamma_2 torch.Size([768])
blocks.3.norm1.weight torch.Size([768])
blocks.3.norm1.bias torch.Size([768])
blocks.3.attn.relative_position_bias_table torch.Size([964, 12])
blocks.3.attn.relative_position_index torch.Size([257, 257])
blocks.3.attn.qkv.weight torch.Size([768, 768])
blocks.3.attn.to_out.0.weight torch.Size([768, 768])
blocks.3.norm2.weight torch.Size([768])
blocks.3.norm2.bias torch.Size([768])
blocks.3.mlp.D torch.Size([768, 768])
blocks.4.gamma_1 torch.Size([768])
blocks.4.gamma_2 torch.Size([768])
blocks.4.norm1.weight torch.Size([768])
blocks.4.norm1.bias torch.Size([768])
blocks.4.attn.relative_position_bias_table torch.Size([964, 12])
blocks.4.attn.relative_position_index torch.Size([257, 257])
blocks.4.attn.qkv.weight torch.Size([768, 768])
blocks.4.attn.to_out.0.weight torch.Size([768, 768])
blocks.4.norm2.weight torch.Size([768])
blocks.4.norm2.bias torch.Size([768])
blocks.4.mlp.D torch.Size([768, 768])
blocks.5.gamma_1 torch.Size([768])
blocks.5.gamma_2 torch.Size([768])
blocks.5.norm1.weight torch.Size([768])
blocks.5.norm1.bias torch.Size([768])
blocks.5.attn.relative_position_bias_table torch.Size([964, 12])
blocks.5.attn.relative_position_index torch.Size([257, 257])
blocks.5.attn.qkv.weight torch.Size([768, 768])
blocks.5.attn.to_out.0.weight torch.Size([768, 768])
blocks.5.norm2.weight torch.Size([768])
blocks.5.norm2.bias torch.Size([768])
blocks.5.mlp.D torch.Size([768, 768])
blocks.6.gamma_1 torch.Size([768])
blocks.6.gamma_2 torch.Size([768])
blocks.6.norm1.weight torch.Size([768])
blocks.6.norm1.bias torch.Size([768])
blocks.6.attn.relative_position_bias_table torch.Size([964, 12])
blocks.6.attn.relative_position_index torch.Size([257, 257])
blocks.6.attn.qkv.weight torch.Size([768, 768])
blocks.6.attn.to_out.0.weight torch.Size([768, 768])
blocks.6.norm2.weight torch.Size([768])
blocks.6.norm2.bias torch.Size([768])
blocks.6.mlp.D torch.Size([768, 768])
blocks.7.gamma_1 torch.Size([768])
blocks.7.gamma_2 torch.Size([768])
blocks.7.norm1.weight torch.Size([768])
blocks.7.norm1.bias torch.Size([768])
blocks.7.attn.relative_position_bias_table torch.Size([964, 12])
blocks.7.attn.relative_position_index torch.Size([257, 257])
blocks.7.attn.qkv.weight torch.Size([768, 768])
blocks.7.attn.to_out.0.weight torch.Size([768, 768])
blocks.7.norm2.weight torch.Size([768])
blocks.7.norm2.bias torch.Size([768])
blocks.7.mlp.D torch.Size([768, 768])
blocks.8.gamma_1 torch.Size([768])
blocks.8.gamma_2 torch.Size([768])
blocks.8.norm1.weight torch.Size([768])
blocks.8.norm1.bias torch.Size([768])
blocks.8.attn.relative_position_bias_table torch.Size([964, 12])
blocks.8.attn.relative_position_index torch.Size([257, 257])
blocks.8.attn.qkv.weight torch.Size([768, 768])
blocks.8.attn.to_out.0.weight torch.Size([768, 768])
blocks.8.norm2.weight torch.Size([768])
blocks.8.norm2.bias torch.Size([768])
blocks.8.mlp.D torch.Size([768, 768])
blocks.9.gamma_1 torch.Size([768])
blocks.9.gamma_2 torch.Size([768])
blocks.9.norm1.weight torch.Size([768])
blocks.9.norm1.bias torch.Size([768])
blocks.9.attn.relative_position_bias_table torch.Size([964, 12])
blocks.9.attn.relative_position_index torch.Size([257, 257])
blocks.9.attn.qkv.weight torch.Size([768, 768])
blocks.9.attn.to_out.0.weight torch.Size([768, 768])
blocks.9.norm2.weight torch.Size([768])
blocks.9.norm2.bias torch.Size([768])
blocks.9.mlp.D torch.Size([768, 768])
blocks.10.gamma_1 torch.Size([768])
blocks.10.gamma_2 torch.Size([768])
blocks.10.norm1.weight torch.Size([768])
blocks.10.norm1.bias torch.Size([768])
blocks.10.attn.relative_position_bias_table torch.Size([964, 12])
blocks.10.attn.relative_position_index torch.Size([257, 257])
blocks.10.attn.qkv.weight torch.Size([768, 768])
blocks.10.attn.to_out.0.weight torch.Size([768, 768])
blocks.10.norm2.weight torch.Size([768])
blocks.10.norm2.bias torch.Size([768])
blocks.10.mlp.D torch.Size([768, 768])
blocks.11.gamma_1 torch.Size([768])
blocks.11.gamma_2 torch.Size([768])
blocks.11.norm1.weight torch.Size([768])
blocks.11.norm1.bias torch.Size([768])
blocks.11.attn.relative_position_bias_table torch.Size([964, 12])
blocks.11.attn.relative_position_index torch.Size([257, 257])
blocks.11.attn.qkv.weight torch.Size([768, 768])
blocks.11.attn.to_out.0.weight torch.Size([768, 768])
blocks.11.norm2.weight torch.Size([768])
blocks.11.norm2.bias torch.Size([768])
blocks.11.mlp.D torch.Size([768, 768])
fpn1.0.weight torch.Size([768, 768, 2, 2])
fpn1.0.bias torch.Size([768])
